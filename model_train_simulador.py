from data_wrangling import *
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBClassifier
import joblib
import os
import pandas as pd


# Importando os datasets
datasets = ["X_train", "X_test", "y_train", "y_test"]
X_train, X_test, y_train, y_test = importando_treino_teste(datasets)

#Removendo a Coluna person_emp_length
if "person_emp_length" in X_train.columns:
    X_train = X_train.drop(columns=["person_emp_length"])
if "person_emp_length" in X_test.columns:
    X_test = X_test.drop(columns=["person_emp_length"])
print("Coluna 'person_emp_length' removida com sucesso!")
print(f"Novas dimensÃµes -> X_train: {X_train.shape}, X_test: {X_test.shape}")

#Verificando se ainda existem colunas com valores nulos
print(X_train.isnull().sum())

#Calculando as taxas mÃ©dias de juros por grau de crÃ©dito
taxas_por_grade = (
    X_train.groupby("loan_grade")["loan_int_rate"]
    .median()  # ou .mean()
    .round(2)
    .to_dict()
)

print("ğŸ“ˆ Taxas medianas por grade:")
for grade, taxa in taxas_por_grade.items():
    print(f"  {grade}: {taxa}%")

# Substitui valores ausentes ou inconsistentes
X_train["loan_int_rate"] = X_train.apply(
    lambda row: taxas_por_grade.get(row["loan_grade"], 12.0)
    if pd.isna(row["loan_int_rate"]) or row["loan_int_rate"] <= 0
    else row["loan_int_rate"],
    axis=1
)

# Aplica o mesmo ajuste para o conjunto de teste
X_test["loan_int_rate"] = X_test.apply(
    lambda row: taxas_por_grade.get(row["loan_grade"], 12.0)
    if pd.isna(row["loan_int_rate"]) or row["loan_int_rate"] <= 0
    else row["loan_int_rate"],
    axis=1
)

# Salva o dicionÃ¡rio de taxas para uso posterior no simulador
os.makedirs("models", exist_ok=True)
joblib.dump(taxas_por_grade, "models/taxas_por_grade.pkl")
print("Tabela de taxas medianas salva em: models/taxas_por_grade.pkl")

#Criando a coluna loan_to_income_ratio
X_train["loan_to_income_ratio"] = X_train["loan_amnt"] / (X_train["person_income"] + 1)
X_test["loan_to_income_ratio"] = X_test["loan_amnt"] / (X_test["person_income"] + 1)

#Substituindo a coluna loan_percent_income pela coluna loan_to_income_ratio
if "loan_percent_income" in X_train.columns:
    X_train = X_train.drop(columns=["loan_percent_income"])
    X_test = X_test.drop(columns=["loan_percent_income"])
    print("Substituindo 'loan_percent_income' por 'loan_to_income_ratio'.")

#identificando colunas
numericas, categoricas = identificar_colunas(X_train)

print("\nğŸ“Š Colunas numÃ©ricas:", numericas)
print("ğŸ“ Colunas categÃ³ricas:", categoricas)

#Montando ColumnTransformer
preprocessor = ColumnTransformer([
    ("num", "passthrough", numericas),
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categoricas)
], remainder="drop")

#Aplicando transformaÃ§Ãµes
X_train_arr = preprocessor.fit_transform(X_train)
X_test_arr  = preprocessor.transform(X_test)

#Recuperando os nomes das colunas
num_cols = list(numericas)
cat_cols = preprocessor.named_transformers_["cat"].get_feature_names_out(categoricas)
todas_cols = num_cols + list(cat_cols)

#Montando os DataFrames pro treino
X_train_ready = pd.DataFrame(X_train_arr, columns=todas_cols, index=X_train.index)
X_test_ready  = pd.DataFrame(X_test_arr,  columns=todas_cols, index=X_test.index)

print(f"\nPrÃ©-processamento finalizado.\nX_train_ready: {X_train_ready.shape}, X_test_ready: {X_test_ready.shape}")
print("Exemplo:")
print(X_train_ready.head())

#Salvando preprocessor e colunas para uso no predict
os.makedirs("models", exist_ok=True)
joblib.dump(preprocessor, "models/preprocessor_simulador.pkl")
joblib.dump(todas_cols, "models/feature_columns_simulador.pkl")
print("\nPreprocessor salvo em models/preprocessor_simulador.pkl")
print("Lista de features salva em models/feature_columns_simulador.pkl")

#CÃ¡lculo automÃ¡tico do peso da classe minoritÃ¡ria
scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]

#InicializaÃ§Ã£o do modelo
xgb_model = XGBClassifier(
    n_estimators=2000,                 #NÃºmero total de Ã¡rvores. Valores maiores reduzem o bias, mas podem gerar overfitting.
    learning_rate=0.08,                #Taxa de aprendizado do modelo. Valores maiores resultam em aprendizado mais rÃ¡pido, porÃ©m geram overfitting
    max_depth=6,                       #Profundidade mÃ¡xima de cada Ã¡rvore. Valores maiores reduzem o bias, mas podem gerar overfitting.
    subsample=0.9,                     #Age como regularizaÃ§Ã£o, ajuda a reduzir a correlaÃ§Ã£o entre Ã¡rvores para reduzir overfitting, valores comuns: 0.6~0.9
    colsample_bytree=0.8,              #Controla o nÃºmero de variÃ¡veis consideradas a cada Ã¡rvore. Evita que algumas features dominem todas as divisÃµes. Valores tÃ­picos: 0.7~0.9
    random_state=42,                   #Random State fixo para reproduÃ§Ã£o futura dos mesmos resultados
    n_jobs=-1,                         #n_jobs=-1: usa todos os nÃºcleos da CPU â€” acelera o treino.
    tree_method="hist",                #Metod interno de construÃ§Ã£o das Ã¡rvores. 'auto': escolha automÃ¡tica. 'hist': usa histogramas â€” mais rÃ¡pido e consome menos memÃ³ria. 'gpu_hist': versÃ£o otimizada para GPU.
    scale_pos_weight=scale_pos_weight, #Corrige a influÃªncia da classe minoritÃ¡ria na funÃ§Ã£o de perda. Melhora recall da classe minoritÃ¡ria.
    base_score=0.5,                    #Define o â€œchute inicialâ€ do modelo.
    eval_metric="aucpr"                #MÃ©trica usada internamente para avaliar o erro durante o treino. 'logloss' â†’ perda logÃ­stica (default para classificaÃ§Ã£o binÃ¡ria). 'auc' â†’ Ã¡rea sob a curva ROC. 'error' â†’ taxa de erro simples
)

#Treinando o Modelo
print("\nTreinando modelo XGBoost...")
xgb_model.fit(X_train_ready, y_train)
print("Modelo treinado com sucesso!")

#Salvando o Modelo
os.makedirs("models", exist_ok=True)
joblib.dump(xgb_model, "models/XGBClassifier_simulador.pkl")
print("Modelo salvo em: models/XGBClassifier_simulador.pkl")