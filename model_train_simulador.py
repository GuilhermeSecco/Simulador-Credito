from data_wrangling import *
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBClassifier
import joblib
import os
import pandas as pd

datasets = ["X_train", "X_test", "y_train", "y_test"]

# Importando os datasets
X_train, X_test, y_train, y_test = importando_treino_teste(datasets)

#Removendo a Coluna person_emp_length
if "person_emp_length" in X_train.columns:
    X_train = X_train.drop(columns=["person_emp_length"])
if "person_emp_length" in X_test.columns:
    X_test = X_test.drop(columns=["person_emp_length"])
print("Coluna 'person_emp_length' removida com sucesso!")
print(f"Novas dimens√µes -> X_train: {X_train.shape}, X_test: {X_test.shape}")

#Verificando se ainda existem colunas com valores nulos
print(X_train.isnull().sum())

#Criando a coluna loan_to_income_ratio
X_train["loan_to_income_ratio"] = X_train["loan_amnt"] / (X_train["person_income"] + 1)
X_test["loan_to_income_ratio"] = X_test["loan_amnt"] / (X_test["person_income"] + 1)

#Substituindo a coluna loan_percent_income pela coluna loan_to_income_ratio
if "loan_percent_income" in X_train.columns:
    X_train = X_train.drop(columns=["loan_percent_income"])
    X_test = X_test.drop(columns=["loan_percent_income"])
    print("Substituindo 'loan_percent_income' por 'loan_to_income_ratio'.")

#identificando colunas
numericas, categoricas = identificar_colunas(X_train)

print("\nüìä Colunas num√©ricas:", numericas)
print("üìÅ Colunas categ√≥ricas:", categoricas)

#Montando ColumnTransformer
preprocessor = ColumnTransformer([
    ("num", "passthrough", numericas),
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categoricas)
], remainder="drop")

#Aplicando transforma√ß√µes
X_train_arr = preprocessor.fit_transform(X_train)
X_test_arr  = preprocessor.transform(X_test)

#Recuperando os nomes das colunas
num_cols = list(numericas)
cat_cols = preprocessor.named_transformers_["cat"].get_feature_names_out(categoricas)
todas_cols = num_cols + list(cat_cols)

#Montando os DataFrames pro treino
X_train_ready = pd.DataFrame(X_train_arr, columns=todas_cols, index=X_train.index)
X_test_ready  = pd.DataFrame(X_test_arr,  columns=todas_cols, index=X_test.index)

print(f"\nPr√©-processamento finalizado.\nX_train_ready: {X_train_ready.shape}, X_test_ready: {X_test_ready.shape}")
print("Exemplo:")
print(X_train_ready.head())

#Salvando preprocessor e colunas para uso no predict
os.makedirs("models", exist_ok=True)
joblib.dump(preprocessor, "models/preprocessor_simulador.pkl")
joblib.dump(todas_cols, "models/feature_columns_simulador.pkl")
print("\nPreprocessor salvo em models/preprocessor_simulador.pkl")
print("Lista de features salva em models/feature_columns_simulador.pkl")

#C√°lculo autom√°tico do peso da classe minorit√°ria
scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]

#Inicializa√ß√£o do modelo
xgb_model = XGBClassifier(
    n_estimators=2000,                 #N√∫mero total de √°rvores. Valores maiores reduzem o bias, mas podem gerar overfitting.
    learning_rate=0.08,                #Taxa de aprendizado do modelo. Valores maiores resultam em aprendizado mais r√°pido, por√©m geram overfitting
    max_depth=6,                       #Profundidade m√°xima de cada √°rvore. Valores maiores reduzem o bias, mas podem gerar overfitting.
    subsample=0.9,                     #Age como regulariza√ß√£o, ajuda a reduzir a correla√ß√£o entre √°rvores para reduzir overfitting, valores comuns: 0.6~0.9
    colsample_bytree=0.8,              #Controla o n√∫mero de vari√°veis consideradas a cada √°rvore. Evita que algumas features dominem todas as divis√µes. Valores t√≠picos: 0.7~0.9
    random_state=42,                   #Random State fixo para reprodu√ß√£o futura dos mesmos resultados
    n_jobs=-1,                         #n_jobs=-1: usa todos os n√∫cleos da CPU ‚Äî acelera o treino.
    tree_method="hist",                #Metod interno de constru√ß√£o das √°rvores. 'auto': escolha autom√°tica. 'hist': usa histogramas ‚Äî mais r√°pido e consome menos mem√≥ria. 'gpu_hist': vers√£o otimizada para GPU.
    scale_pos_weight=scale_pos_weight, #Corrige a influ√™ncia da classe minorit√°ria na fun√ß√£o de perda. Melhora recall da classe minorit√°ria.
    base_score=0.5,                    #Define o ‚Äúchute inicial‚Äù do modelo.
    eval_metric="aucpr"              #M√©trica usada internamente para avaliar o erro durante o treino. 'logloss' ‚Üí perda log√≠stica (default para classifica√ß√£o bin√°ria). 'auc' ‚Üí √°rea sob a curva ROC. 'error' ‚Üí taxa de erro simples
)

#Treinando o Modelo
print("\nTreinando modelo XGBoost...")
xgb_model.fit(X_train_ready, y_train)
print("Modelo treinado com sucesso!")

#Salvando o Modelo
os.makedirs("models", exist_ok=True)
joblib.dump(xgb_model, "models/XGBClassifier_simulador.pkl")
print("üíæ Modelo salvo em: models/XGBClassifier_simulador.pkl")